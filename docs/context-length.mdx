---
title: Context length
---

Context length is the maximum number of tokens that the model has access to in memory.  

<Note>
  The default context length in Ollama is 4096 tokens.
</Note>

Tasks which require large context like web search, agents, and coding tools should be set to at least 32000 tokens.

## Setting context length

Setting a larger context length will increase the amount of memory required to run a model. Ensure you have enough VRAM available to increase the context length.

Cloud models are set to their maximum context length by default.

### App

Change the slider in the Ollama app under settings to your desired context length.
![Context length in Ollama app](./images/ollama-settings.png)

### CLI

#### Ollama Serve
If editing the context length for Ollama is not possible, the context length can also be updated when serving Ollama.
```
OLLAMA_CONTEXT_LENGTH=32000 ollama serve
```

#### Chat Mode
The context length can also be adjusted while running a model interactively.

Start the model:
```
ollama run <model>
```

Inside the interactive prompt, set the context length:
```
/set parameter num_ctx 65536
```

This change applies only to the current session.

#### Saving the context permanently
To persist the updated context length, save the model configuration under a new name, for example:
```
/save model_32k
```

Use the saved model name from then on:
```
ollama run model_32k
```

### Check allocated context length and model offloading
For best performance, use the maximum context length for a model, and avoid offloading the model to CPU. Verify the split under `PROCESSOR` using `ollama ps`.
```
ollama ps
```
```
NAME             ID              SIZE      PROCESSOR    CONTEXT    UNTIL
gemma3:latest    a2af6cc3eb7f    6.6 GB    100% GPU     65536      2 minutes from now
```
